15:04:02.628 main INFO SparkContext: Running Spark version 2.1.0
15:04:02.954 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15:04:03.085 main INFO SecurityManager: Changing view acls to: alex
15:04:03.086 main INFO SecurityManager: Changing modify acls to: alex
15:04:03.086 main INFO SecurityManager: Changing view acls groups to: 
15:04:03.087 main INFO SecurityManager: Changing modify acls groups to: 
15:04:03.088 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(alex); groups with view permissions: Set(); users  with modify permissions: Set(alex); groups with modify permissions: Set()
15:04:03.457 main INFO Utils: Successfully started service 'sparkDriver' on port 40772.
15:04:03.479 main INFO SparkEnv: Registering MapOutputTracker
15:04:03.497 main INFO SparkEnv: Registering BlockManagerMaster
15:04:03.502 main INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:04:03.502 main INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
15:04:03.513 main INFO DiskBlockManager: Created local directory at /tmp/blockmgr-84627ba0-f7f2-4037-8671-bdd95976c573
15:04:03.531 main INFO MemoryStore: MemoryStore started with capacity 1062.2 MB
15:04:03.577 main INFO SparkEnv: Registering OutputCommitCoordinator
15:04:03.689 main INFO log: Logging initialized @2150ms
15:04:03.761 main INFO Server: jetty-9.2.z-SNAPSHOT
15:04:03.777 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c18016b{/jobs,null,AVAILABLE}
15:04:03.777 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@33aeca0b{/jobs/json,null,AVAILABLE}
15:04:03.777 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@43aaf813{/jobs/job,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57ac5227{/jobs/job/json,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ba302e0{/stages,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e98770d{/stages/json,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1ae67cad{/stages/stage,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2f6e28bc{/stages/stage/json,null,AVAILABLE}
15:04:03.778 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7c098bb3{/stages/pool,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31e4bb20{/stages/pool/json,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18cebaa5{/storage,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@463b4ac8{/storage/json,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@765f05af{/storage/rdd,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62f68dff{/storage/rdd/json,null,AVAILABLE}
15:04:03.779 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f001896{/environment,null,AVAILABLE}
15:04:03.784 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13f17eb4{/environment/json,null,AVAILABLE}
15:04:03.785 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1d0d6318{/executors,null,AVAILABLE}
15:04:03.785 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4bc28c33{/executors/json,null,AVAILABLE}
15:04:03.785 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4409e975{/executors/threadDump,null,AVAILABLE}
15:04:03.785 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c153b9e{/executors/threadDump/json,null,AVAILABLE}
15:04:03.790 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a7686a7{/static,null,AVAILABLE}
15:04:03.790 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@758a34ce{/,null,AVAILABLE}
15:04:03.791 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7ec3394b{/api,null,AVAILABLE}
15:04:03.791 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@bff34c6{/jobs/job/kill,null,AVAILABLE}
15:04:03.791 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1522d8a0{/stages/stage/kill,null,AVAILABLE}
15:04:03.795 main INFO ServerConnector: Started ServerConnector@1cbf6e72{HTTP/1.1}{0.0.0.0:4040}
15:04:03.796 main INFO Server: Started @2257ms
15:04:03.796 main INFO Utils: Successfully started service 'SparkUI' on port 4040.
15:04:03.797 main INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.141.147:4040
15:04:03.877 main INFO SparkContext: Added file file:///data/oap/oap-new.jar at file:///data/oap/oap-new.jar with timestamp 1528268643876
15:04:03.879 main INFO Utils: Copying /data/oap/oap-new.jar to /tmp/spark-953b1762-57cd-4445-b37c-02b930a66a31/userFiles-1667e781-60bd-4f51-bcd0-f3365a156b30/oap-new.jar
15:04:03.970 main INFO Executor: Starting executor ID driver on host localhost
15:04:03.987 main INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36787.
15:04:03.987 main INFO NettyBlockTransferService: Server created on 192.168.141.147:36787
15:04:03.989 main INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:04:03.990 main INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.141.147, 36787, None)
15:04:03.993 dispatcher-event-loop-2 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.141.147:36787 with 1062.2 MB RAM, BlockManagerId(driver, 192.168.141.147, 36787, None)
15:04:03.994 main INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.141.147, 36787, None)
15:04:03.995 main INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.141.147, 36787, None)
15:04:04.138 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ecf9fb3{/metrics/json,null,AVAILABLE}
15:04:04.244 main INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://master:9000/user/hive/warehouse').
15:04:04.244 main INFO SharedState: Warehouse path is 'hdfs://master:9000/user/hive/warehouse'.
15:04:04.253 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@271f18d3{/SQL,null,AVAILABLE}
15:04:04.254 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@61e3a1fd{/SQL/json,null,AVAILABLE}
15:04:04.255 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3fc08eec{/SQL/execution,null,AVAILABLE}
15:04:04.255 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7b02e036{/SQL/execution/json,null,AVAILABLE}
15:04:04.257 main INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1bb9aa43{/static/sql,null,AVAILABLE}
15:04:04.651 main INFO SparkSqlParser: Parsing command: CREATE TEMPORARY VIEW temp (c_netnum int,
c_ip                	bigint,
c_event_id          	bigint,
c_priority          	int,
c_time              	string,
c_flowid            	string,
c_src_ipv4          	bigint,
c_src_ipv6          	string,
c_src_port          	int,
c_s_tunnel_ip       	bigint,
c_s_tunnel_port     	int,
c_dest_ipv4         	bigint,
c_dest_ipv6         	string,
c_dest_port         	int,
c_d_tunnel_ip       	bigint,
c_d_tunnel_port     	int,
c_proto_type        	int,
c_return_info       	string,
c_s_boundary        	bigint,
c_s_region          	bigint,
c_s_city            	bigint,
c_s_district        	bigint,
c_s_operators       	bigint,
c_s_owner           	string,
c_d_boundary        	bigint,
c_d_region          	bigint,
c_d_city            	bigint,
c_d_district        	bigint,
c_d_operators       	bigint,
c_d_owner           	string,
c_ret_file_type     	int,
c_ret_filename      	string,
c_ret_file          	string,
c_url               	string,
c_cookie            	string,
c_time_part string)
 USING parquet
 OPTIONS (path "hdfs://master:9000/data/oap/")
15:04:06.925 main INFO SparkSqlParser: Parsing command: drop oindex index1 on temp
15:04:06.944 main INFO DropIndex: Dropping index index1
15:04:07.443 main INFO SparkSqlParser: Parsing command: SELECT count(*) FROM temp where c_event_id>30000 and c_event_id<40000
15:04:07.774 main INFO FileSourceStrategy: Pruning directories with: 
15:04:07.788 main INFO FileSourceStrategy: hasAvailableIndex = false, will retain ParquetFileFormat.
15:04:07.792 main INFO FileSourceStrategy: Post-Scan Filters: isnotnull(c_event_id#2L),(c_event_id#2L > 30000),(c_event_id#2L < 40000)
15:04:07.794 main INFO FileSourceStrategy: Output Data Schema: struct<c_event_id: bigint>
15:04:07.797 main INFO FileSourceStrategy: Pushed Filters: IsNotNull(c_event_id),GreaterThan(c_event_id,30000),LessThan(c_event_id,40000)
15:04:07.835 main WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
15:04:08.168 main INFO CodeGenerator: Code generated in 253.521511 ms
15:04:08.216 main INFO CodeGenerator: Code generated in 26.68666 ms
15:04:08.369 main INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 139.2 KB, free 922.1 MB)
15:04:08.414 main INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.6 KB, free 922.0 MB)
15:04:08.417 dispatcher-event-loop-0 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.141.147:36787 (size: 15.6 KB, free: 1062.2 MB)
15:04:08.421 main INFO SparkContext: Created broadcast 0 from collect at TestReadFile.scala:9
15:04:08.427 main INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
15:04:08.542 main INFO SparkContext: Starting job: collect at TestReadFile.scala:9
15:04:08.552 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 4 (collect at TestReadFile.scala:9)
15:04:08.554 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at TestReadFile.scala:9) with 1 output partitions
15:04:08.554 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at TestReadFile.scala:9)
15:04:08.555 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15:04:08.556 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15:04:08.560 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at collect at TestReadFile.scala:9), which has no missing parents
15:04:08.613 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.8 KB, free 922.0 MB)
15:04:08.616 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KB, free 922.0 MB)
15:04:08.617 dispatcher-event-loop-1 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.141.147:36787 (size: 6.4 KB, free: 1062.2 MB)
15:04:08.618 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
15:04:08.621 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at collect at TestReadFile.scala:9)
15:04:08.622 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15:04:08.667 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 6735 bytes)
15:04:08.673 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15:04:08.676 Executor task launch worker-0 INFO Executor: Fetching file:///data/oap/oap-new.jar with timestamp 1528268643876
15:04:08.717 Executor task launch worker-0 INFO Utils: /data/oap/oap-new.jar has been previously copied to /tmp/spark-953b1762-57cd-4445-b37c-02b930a66a31/userFiles-1667e781-60bd-4f51-bcd0-f3365a156b30/oap-new.jar
15:04:08.769 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00002-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-57187477, partition values: [empty row]
15:04:09.078 Executor task launch worker-0 INFO CodecPool: Got brand-new decompressor [.snappy]
15:04:09.183 Spark Context Cleaner INFO ContextCleaner: Cleaned accumulator 0
15:04:09.202 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00000-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-54078512, partition values: [empty row]
15:04:09.252 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2285 bytes result sent to driver
15:04:09.256 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 6735 bytes)
15:04:09.257 Executor task launch worker-0 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15:04:09.261 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 617 ms on localhost (executor driver) (1/2)
15:04:09.262 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00001-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-52436128, partition values: [empty row]
15:04:09.287 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00003-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-7824019, partition values: [empty row]
15:04:09.309 Executor task launch worker-0 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15:04:09.312 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 59 ms on localhost (executor driver) (2/2)
15:04:09.313 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15:04:09.317 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 0 (collect at TestReadFile.scala:9) finished in 0.684 s
15:04:09.318 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
15:04:09.318 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
15:04:09.319 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 1)
15:04:09.319 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
15:04:09.324 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at collect at TestReadFile.scala:9), which has no missing parents
15:04:09.328 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 922.0 MB)
15:04:09.329 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 922.0 MB)
15:04:09.330 dispatcher-event-loop-3 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.141.147:36787 (size: 3.7 KB, free: 1062.2 MB)
15:04:09.331 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
15:04:09.332 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at collect at TestReadFile.scala:9)
15:04:09.332 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15:04:09.336 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 5923 bytes)
15:04:09.336 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
15:04:09.354 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15:04:09.356 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15:04:09.366 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2042 bytes result sent to driver
15:04:09.367 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 34 ms on localhost (executor driver) (1/1)
15:04:09.367 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15:04:09.368 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at TestReadFile.scala:9) finished in 0.035 s
15:04:09.373 main INFO DAGScheduler: Job 0 finished: collect at TestReadFile.scala:9, took 0.830684 s
15:04:09.394 main INFO CodeGenerator: Code generated in 8.702419 ms
15:04:09.403 main INFO SparkSqlParser: Parsing command: create oindex index1 on temp (c_event_id)
15:04:09.410 main INFO CreateIndex: Creating index index1
15:04:09.446 main INFO FileSourceStrategy: Pruning directories with: 
15:04:09.459 main INFO FileSourceStrategy: hasAvailableIndex = false, will retain ParquetFileFormat.
15:04:09.460 main INFO FileSourceStrategy: Post-Scan Filters: 
15:04:09.460 main INFO FileSourceStrategy: Output Data Schema: struct<c_event_id: bigint>
15:04:09.460 main INFO FileSourceStrategy: Pushed Filters: 
15:04:09.463 main INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15:04:09.463 main INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15:04:09.463 main INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15:04:09.463 main INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15:04:09.463 main INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15:04:09.465 main INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
15:04:09.478 main INFO CodeGenerator: Code generated in 10.421665 ms
15:04:09.484 main INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 139.2 KB, free 921.9 MB)
15:04:09.492 main INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.6 KB, free 921.9 MB)
15:04:09.493 dispatcher-event-loop-3 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.141.147:36787 (size: 15.6 KB, free: 1062.2 MB)
15:04:09.493 main INFO SparkContext: Created broadcast 3 from sql at TestReadFile.scala:83
15:04:09.494 main INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
15:04:09.515 main INFO SparkContext: Starting job: sql at TestReadFile.scala:83
15:04:09.516 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (sql at TestReadFile.scala:83) with 2 output partitions
15:04:09.516 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (sql at TestReadFile.scala:83)
15:04:09.516 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
15:04:09.516 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
15:04:09.516 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at sql at TestReadFile.scala:83), which has no missing parents
15:04:09.541 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 57.1 KB, free 921.8 MB)
15:04:09.543 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.4 KB, free 921.8 MB)
15:04:09.544 dispatcher-event-loop-0 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.141.147:36787 (size: 21.4 KB, free: 1062.1 MB)
15:04:09.544 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
15:04:09.545 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sql at TestReadFile.scala:83)
15:04:09.545 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
15:04:09.547 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 6754 bytes)
15:04:09.547 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
15:04:09.557 Executor task launch worker-0 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
15:04:09.557 Executor task launch worker-0 INFO deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
15:04:09.557 Executor task launch worker-0 INFO deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
15:04:09.570 Executor task launch worker-0 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
15:04:09.577 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00002-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-57187477, partition values: [empty row]
15:04:09.614 Executor task launch worker-0 INFO CodeGenerator: Code generated in 7.631298 ms
15:04:09.633 Executor task launch worker-0 INFO CodeGenerator: Code generated in 6.484264 ms
15:04:09.652 Executor task launch worker-0 INFO CodeGenerator: Code generated in 13.238389 ms
15:04:09.790 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00000-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-54078512, partition values: [empty row]
15:04:10.472 dispatcher-event-loop-1 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.141.147:36787 in memory (size: 6.4 KB, free: 1062.1 MB)
15:04:10.475 dispatcher-event-loop-1 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.141.147:36787 in memory (size: 3.7 KB, free: 1062.1 MB)
15:04:11.022 Executor task launch worker-0 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20180606150409_0002_m_000000_0
15:04:11.026 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2410 bytes result sent to driver
15:04:11.028 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 6754 bytes)
15:04:11.029 Executor task launch worker-0 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
15:04:11.030 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1485 ms on localhost (executor driver) (1/2)
15:04:11.039 Executor task launch worker-0 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
15:04:11.039 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00001-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-52436128, partition values: [empty row]
15:04:11.109 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00003-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-7824019, partition values: [empty row]
15:04:12.055 Executor task launch worker-0 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20180606150411_0002_m_000001_0
15:04:12.057 Executor task launch worker-0 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 2337 bytes result sent to driver
15:04:12.058 task-result-getter-0 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1031 ms on localhost (executor driver) (2/2)
15:04:12.058 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15:04:12.059 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (sql at TestReadFile.scala:83) finished in 2.514 s
15:04:12.059 main INFO DAGScheduler: Job 1 finished: sql at TestReadFile.scala:83, took 2.544240 s
15:04:12.072 main INFO FileFormatWriter: Job null committed.
15:04:12.093 main INFO SparkSqlParser: Parsing command: SELECT count(*) FROM temp where c_event_id>30000 and c_event_id<40000
15:04:12.121 main INFO FileSourceStrategy: Pruning directories with: 
15:04:12.128 main INFO FileSourceStrategy: hasAvailableIndex = true, will replace with OapFileFormat.
15:04:12.130 main INFO FileSourceStrategy: Post-Scan Filters: isnotnull(c_event_id#2L),(c_event_id#2L > 30000),(c_event_id#2L < 40000)
15:04:12.130 main INFO FileSourceStrategy: Output Data Schema: struct<c_event_id: bigint>
15:04:12.130 main INFO FileSourceStrategy: Pushed Filters: IsNotNull(c_event_id),GreaterThan(c_event_id,30000),LessThan(c_event_id,40000)
15:04:12.150 main INFO CodeGenerator: Code generated in 10.212924 ms
15:04:12.173 main INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 138.9 KB, free 921.7 MB)
15:04:12.178 main INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.4 KB, free 921.7 MB)
15:04:12.179 dispatcher-event-loop-1 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.141.147:36787 (size: 15.4 KB, free: 1062.1 MB)
15:04:12.180 main INFO SparkContext: Created broadcast 5 from collect at TestReadFile.scala:9
15:04:12.180 main INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
15:04:12.191 main INFO SparkContext: Starting job: collect at TestReadFile.scala:9
15:04:12.196 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 13 (collect at TestReadFile.scala:9)
15:04:12.196 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at TestReadFile.scala:9) with 1 output partitions
15:04:12.196 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (collect at TestReadFile.scala:9)
15:04:12.196 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15:04:12.196 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15:04:12.197 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[13] at collect at TestReadFile.scala:9), which has no missing parents
15:04:12.219 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 15.2 KB, free 921.7 MB)
15:04:12.220 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.3 KB, free 921.6 MB)
15:04:12.220 dispatcher-event-loop-0 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.141.147:36787 (size: 7.3 KB, free: 1062.1 MB)
15:04:12.221 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
15:04:12.221 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[13] at collect at TestReadFile.scala:9)
15:04:12.221 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
15:04:12.223 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, ANY, 6735 bytes)
15:04:12.223 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)
15:04:12.229 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00002-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-57187477, partition values: [empty row]
15:04:12.248 Executor task launch worker-0 INFO BPlusTreeScanner: Partition File hdfs://master:9000/data/oap/part-00002-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet will use OAP index.

15:04:12.343 Executor task launch worker-0 INFO ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int64 c_event_id;
}

Catalyst form:
StructType(StructField(c_event_id,LongType,true))
       
15:04:12.447 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00000-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-54078512, partition values: [empty row]
15:04:12.458 Executor task launch worker-0 INFO BPlusTreeScanner: Partition File hdfs://master:9000/data/oap/part-00000-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet will use OAP index.

15:04:12.490 Executor task launch worker-0 INFO ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int64 c_event_id;
}

Catalyst form:
StructType(StructField(c_event_id,LongType,true))
       
15:04:12.515 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 2034 bytes result sent to driver
15:04:12.516 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6, localhost, executor driver, partition 1, ANY, 6735 bytes)
15:04:12.516 Executor task launch worker-0 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)
15:04:12.516 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 294 ms on localhost (executor driver) (1/2)
15:04:12.518 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00001-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-52436128, partition values: [empty row]
15:04:12.528 Executor task launch worker-0 INFO BPlusTreeScanner: Partition File hdfs://master:9000/data/oap/part-00001-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet will use OAP index.

15:04:12.555 Executor task launch worker-0 INFO ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int64 c_event_id;
}

Catalyst form:
StructType(StructField(c_event_id,LongType,true))
       
15:04:12.587 Executor task launch worker-0 INFO FileScanRDD: Reading File path: hdfs://master:9000/data/oap/part-00003-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet, range: 0-7824019, partition values: [empty row]
15:04:12.600 Executor task launch worker-0 INFO BPlusTreeScanner: Partition File hdfs://master:9000/data/oap/part-00003-d64bad70-e5ac-4283-bcd5-a3270314572e.snappy.parquet will use OAP index.

15:04:12.616 Executor task launch worker-0 INFO ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int64 c_event_id;
}

Catalyst form:
StructType(StructField(c_event_id,LongType,true))
       
15:04:12.628 Executor task launch worker-0 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 2034 bytes result sent to driver
15:04:12.630 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 115 ms on localhost (executor driver) (2/2)
15:04:12.630 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15:04:12.633 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 3 (collect at TestReadFile.scala:9) finished in 0.412 s
15:04:12.633 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
15:04:12.633 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
15:04:12.633 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 4)
15:04:12.633 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
15:04:12.634 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at collect at TestReadFile.scala:9), which has no missing parents
15:04:12.635 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.0 KB, free 921.6 MB)
15:04:12.637 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KB, free 921.6 MB)
15:04:12.638 dispatcher-event-loop-2 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.141.147:36787 (size: 3.7 KB, free: 1062.1 MB)
15:04:12.639 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
15:04:12.640 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at collect at TestReadFile.scala:9)
15:04:12.640 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15:04:12.641 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7, localhost, executor driver, partition 0, ANY, 5923 bytes)
15:04:12.643 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)
15:04:12.646 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15:04:12.646 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15:04:12.649 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 2042 bytes result sent to driver
15:04:12.652 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 12 ms on localhost (executor driver) (1/1)
15:04:12.652 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15:04:12.653 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (collect at TestReadFile.scala:9) finished in 0.013 s
15:04:12.654 main INFO DAGScheduler: Job 2 finished: collect at TestReadFile.scala:9, took 0.462847 s
15:04:12.659 Thread-1 INFO SparkContext: Invoking stop() from shutdown hook
15:04:12.674 Thread-1 INFO ServerConnector: Stopped ServerConnector@1cbf6e72{HTTP/1.1}{0.0.0.0:4040}
15:04:12.676 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1522d8a0{/stages/stage/kill,null,UNAVAILABLE}
15:04:12.676 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@bff34c6{/jobs/job/kill,null,UNAVAILABLE}
15:04:12.676 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@7ec3394b{/api,null,UNAVAILABLE}
15:04:12.676 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@758a34ce{/,null,UNAVAILABLE}
15:04:12.676 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a7686a7{/static,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c153b9e{/executors/threadDump/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4409e975{/executors/threadDump,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4bc28c33{/executors/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1d0d6318{/executors,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@13f17eb4{/environment/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@f001896{/environment,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@62f68dff{/storage/rdd/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@765f05af{/storage/rdd,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@463b4ac8{/storage/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@18cebaa5{/storage,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@31e4bb20{/stages/pool/json,null,UNAVAILABLE}
15:04:12.677 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@7c098bb3{/stages/pool,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f6e28bc{/stages/stage/json,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1ae67cad{/stages/stage,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@e98770d{/stages/json,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4ba302e0{/stages,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@57ac5227{/jobs/job/json,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@43aaf813{/jobs/job,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@33aeca0b{/jobs/json,null,UNAVAILABLE}
15:04:12.678 Thread-1 INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c18016b{/jobs,null,UNAVAILABLE}
15:04:12.680 Thread-1 INFO SparkUI: Stopped Spark web UI at http://192.168.141.147:4040
15:04:12.691 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15:04:12.703 Thread-1 INFO MemoryStore: MemoryStore cleared
15:04:12.703 Thread-1 INFO BlockManager: BlockManager stopped
15:04:12.705 Thread-1 INFO BlockManagerMaster: BlockManagerMaster stopped
15:04:12.707 dispatcher-event-loop-3 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15:04:12.716 Thread-1 INFO SparkContext: Successfully stopped SparkContext
15:04:12.716 Thread-1 INFO ShutdownHookManager: Shutdown hook called
15:04:12.717 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-953b1762-57cd-4445-b37c-02b930a66a31
